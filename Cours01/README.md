# Rappels sur l'apprentissage par renforcement

Ce premier cours porte sur les notions essentielles d'apprentissage par renforcement. Il n'est pas encore question d'apprentissage *profond*.

Les principales notions abordées sont :

* Les Processus de Décision Markoviens ; Définition d'un agent, d'un environnement, d'un état et de la récompense.
* Définition des retours, d'une fonction de valeur, d'une politique
* Équation de Bellman
* Compromis exploration/exploitation ; Problème des k-bandits

Le fichier `Esperance.pdf` complète ce cours par une introdution à l'espérance en probabilités.

Des exercices pratiques sur un environnement de type *gridworld* concluent ce cours. En plus de mettre en application les notions du cours, ces exercices seront l'occasion d'introduire les classes python qui seront utilisées tout au long du semestre pour développer des agents.

Si vous ne disposez pas d'une installation de Jupyter sur votre poste pour suivre le cours, vous pouvez ouvrir le notebook [sur Google Colab (interactif)](https://colab.research.google.com/github/pcouy/ESGI-M2-IABD/blob/main/Cours01/Rappels.ipynb) ou [sur Jupyter NBViewer (lecture seule)](https://nbviewer.jupyter.org/github/pcouy/ESGI-M2-IABD/blob/main/Cours01/Rappels.ipynb). Les blocs d'équation sur plusieurs lignes s'affichent mal sur l'apperçu généré par Github.
