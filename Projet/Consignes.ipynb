{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "proj-IABD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Projet Deep Reinforcement Learning - M2 IABD\n",
        "\n",
        "Avant toute chose, vérifiez que vous avez bien un GPU (sous Colab : Exécution > Modifier le type d'exécution > Accélérateur matériel)\n",
        "\n",
        "Puis lancez la cellule suivante qui va connecter votre google drive sous Colab et définir le dossier de travail."
      ],
      "metadata": {
        "id": "lUyfBkUC1QVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/gdrive')\n",
        "    import os\n",
        "    os.makedirs('/gdrive/My Drive/IABD-DeepRL', exist_ok=True)\n",
        "    data_dir = '/gdrive/My Drive/IABD-DeepRL'\n",
        "else:\n",
        "    data_dir = '.'\n",
        "%cd $data_dir"
      ],
      "metadata": {
        "id": "6aZIlQT6ceiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Lancez ensuite la cellule suivante qui met à jour le dépôt et installe les dépendances. Cette cellule peut prendre un certain temps à s'exécuter (surtout la 1ère fois) :"
      ],
      "metadata": {
        "id": "CGEhvWf1ejxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pcouy/ESGI-M2-IABD\n",
        "!cd ESGI-M2-IABD && git pull\n",
        "!pip install -U pip\n",
        "!pip install ESGI-M2-IABD/code/"
      ],
      "metadata": {
        "id": "XUgGmGSR1iFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le fait de choisir un dossier de travail dans votre Drive vous permet de rendre persistantes les modifications que vous apporterez au code fourni.\n",
        "\n",
        "La cellule suivante fournit quelques fonctions utilitaires qui sont utilisées pour afficher les vidéos des épisodes de test dans le *notebook* :"
      ],
      "metadata": {
        "id": "qU01tlY-3R1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonctions utilitaires pour afficher interactivement les vidéos des épisodes de test\n",
        "# ATTENTION : Le slider de view_videos ne fonctionne que s'il n'y a pas d'exécution déjà en cours !\n",
        "import os, io, glob, base64, threading\n",
        "from ipywidgets import interact, widgets\n",
        "from IPython.display import HTML, Image, display\n",
        "\n",
        "def load_and_display(path):\n",
        "    \"\"\"Show a video at `path` within IPython Notebook.\"\"\"\n",
        "    if not os.path.isfile(path):\n",
        "        raise NameError(\"Cannot access: {}\".format(path))\n",
        "\n",
        "    video = io.open(path, \"r+b\").read()\n",
        "    encoded = base64.b64encode(video)\n",
        "\n",
        "    display(HTML(\n",
        "        data=\"\"\"\n",
        "        <video alt=\"test\" controls>\n",
        "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\"/>\n",
        "        </video>\n",
        "        \"\"\".format(encoded.decode(\"ascii\"))\n",
        "    ))\n",
        "\n",
        "def show_last_video(agent_save_dir):\n",
        "    dirname = os.path.join(agent_save_dir, \"videos\")\n",
        "    files = sorted([f for f in os.listdir(dirname)])\n",
        "    file = files[-1]\n",
        "    path = os.path.join(dirname, file, \"rl-video-episode-0.mp4\")\n",
        "    load_and_display(path)\n",
        "\n",
        "def view_videos(agent):\n",
        "    dirname = os.path.join(agent.save_dir, \"videos\")\n",
        "    files = sorted([f for f in os.listdir(dirname)])\n",
        "    print(files)\n",
        "    N = len(files)\n",
        "    def d(file: str) -> None:\n",
        "        path = os.path.join(dirname, file, \"rl-video-episode-0.mp4\")\n",
        "        load_and_display(path)\n",
        "    interact(d, file=widgets.SelectionSlider(options=files, value=files[-1]))"
      ],
      "metadata": {
        "id": "TToLPODd19AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objectif du projet\n",
        "\n",
        "L'objectif de ce projet est la mise en oeuvre des modèles de *Deep Reinforcement Learning* vus en cours.\n",
        "\n",
        "Vous utiliserez dans un premier temps le code fourni pour créer plusieurs agents et comparer leurs performances.\n",
        "\n",
        "La seconde partie de ce travail consitera à reproduire vous-même les résultats d'un ou plusieurs articles scientifiques de votre choix en vous appuyant sur le code fourni et les notions vues en cours.\n",
        "\n",
        "Pour ces deux parties, vous devrez produire un compte rendu présentant les résultats de vos expériences, accompagné du code permettant de reproduire vos résultats.\n",
        "\n",
        "Pour l'ensemble du projet, vous évaluerez vos agent sur l'environnement *Snake* 8x8. Bien qu'il soit possible d'appliquer les algorithmes vus ici à d'autres environnements plus complexes, il est recommandé de s'en tenir au jeu *Snake* afin d'obtenir des résultats en quelques heures (les environnements *Atari* nécessitant par exemple plusieurs jours d'entrainement).\n",
        "\n",
        "### Conseils\n",
        "\n",
        "* Testez vos implémentations sur des problèmes plus simples (le *Snake* 4x4 par exemple) pour gagner du temps sur le débuggage\n",
        "* Testez les valeurs des hyper-paramètres en partant des valeurs données dans les articles scientifiques de référence, et ajustez les progressivement. Le code fourni enregistre les résultats de chaque expérience, accompagné des hyper-paramètres utilisés. Utilisez cet historique à votre avantage pour accéler votre recherche.\n",
        "* Répartissez bien les tâches entre les membres de votre groupe. Chaque expérience coûte un certain temps, ne le gaspillez pas en re-faisant des expériences déjà tentées par vos collègues.\n",
        "* Google limite malheureusement le temps de GPU par compte sur Colab. Vous pouvez contourner cette limite si vos disposez de plusieurs comptes Google\n",
        "* Il peut parfois être utile de surveiller l'évolution d'une expérience pour pouvoir l'arrêter prématurément lorsque celle ci est manifestement un échec. Attention toutefois à ne pas perdre des heures à surveiller l'apprentissage de votre agent.\n",
        "* Vous n'aurez pas le temps de trouver vous-même de bonnes valeurs pour tous les hyper-paramètres, c'est un processus très lent. N'hésitez pas à utiliser toutes les ressources que vous pourrez trouver en ligne pour vous y aider."
      ],
      "metadata": {
        "id": "xoqRuZ6n8Oyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction - DQN Simple\n",
        "\n",
        "La cellule ci-dessous créé une classe héritant de 3 autres, qui ajoute à la fois le *replay* d'expérience et la fonction de valeur cible au `QLearningAgent`.\n",
        "\n",
        "Ce sont ces modifications qui permettent d'utiliser un réseau de neurone pour approximer la fonction de valeur."
      ],
      "metadata": {
        "id": "letAsK5LGy3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import code_tp as TP\n",
        "from code_tp import agents, value_functions, policies\n",
        "from code_tp import wrappers\n",
        "import gym\n",
        "\n",
        "# La déclaration de classe ci-dessous permet de combiner dans\n",
        "# la classe `DQNAgent` les amélirations apportées par les 3\n",
        "# classes dont elle hérite. Notez particulièrement la section\n",
        "# `Method resolution order` dans la sortie de cette cellule.\n",
        "class DQNAgent(agents.target_value.TargetValueAgent, agents.replay_buffer.ReplayBufferAgent, agents.base.QLearningAgent):\n",
        "    pass\n",
        "help(DQNAgent)"
      ],
      "metadata": {
        "id": "s-qi_6f9KkRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous pouvons mantenant créer l'environnement *Snake* et instancier l'agent. Les valeurs des paramètres donnés ci-dessous ne sont *a priori* pas optimaux mais fonctionnent bien pour le *Snake* en pratique."
      ],
      "metadata": {
        "id": "045x9ntkMzMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "env = gym.make(\"Snake-8x8-v0\")\n",
        "env = gym.wrappers.FrameStack(env,2)\n",
        "env = wrappers.utils.TimeToChannels(env)\n",
        "env = wrappers.utils.BoredomWrapper(env, -0.0005)\n",
        "\n",
        "a=TP.create_agent_from_env(env,\n",
        "agent_class=DQNAgent,\n",
        "value_class=value_functions.neural.ConvolutionalQFunction,\n",
        "policy_class=policies.greedy.EGreedyPolicy,\n",
        "agent_args={\n",
        "    'gamma':0.99, 'replay_buffer_class':agents.replay_buffer.ReplayBuffer,\n",
        "    'replay_buffer_args': {\n",
        "        'batch_size':32,\n",
        "        'max_size':150000\n",
        "    },\n",
        "    'target_update': 1000,\n",
        "    'update_interval': 1,\n",
        "    'save_dir': os.path.join(data_dir, \"results\")+\"/\"\n",
        "},\n",
        "value_args={\n",
        "    'lr':1e-3,\n",
        "    'nn_args': {\n",
        "        'kernel_size':[1,3,3,3],\n",
        "        'stride':1,\n",
        "        'n_filters':[4,16,32,64],\n",
        "        'padding':1,\n",
        "        'pooling':[None,None,None,None],\n",
        "        'activation': nn.ReLU,\n",
        "        'output_stack_class': value_functions.neural_nets.linear.LinearNeuralStack,\n",
        "        'output_stack_args': {\n",
        "            'layers': [256]\n",
        "        }\n",
        "    }\n",
        "},\n",
        "policy_args={\n",
        "    'greedy_policy_class': policies.greedy.GreedyQPolicy,\n",
        "    'epsilon': 0.9, 'epsilon_decay': 2e-5, 'epsilon_min': 0.02,\n",
        "    'epsilon_test':0\n",
        "})\n"
      ],
      "metadata": {
        "id": "7Onym91fLaA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On charge à présent [*Tensorboard*](https://www.tensorflow.org/tensorboard) qui est un outil permettant de visualiser les courbes d'entrainement en direct et de comparer les résultats de plusieurs expériences :"
      ],
      "metadata": {
        "id": "xbbwmuCKmTYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir results --samples_per_plugin scalars=50000"
      ],
      "metadata": {
        "id": "sgLPBuaUmwId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les courbes de vos entrainements passés et en cours s'afficheront dans Tensorboard. Vous pouvez sélectionner les expériences que vous souhaitez afficher. L'onglet \"Time Series\" a l'avantage de mettre à jour automatiquement l'échelle des courbes pour afficher les nouvelles données.\n",
        "\n",
        "Il est recommandé d'utiliser une valeur de 0.99 pour le paramètre \"smoothing\" dans Tensorboard.\n",
        "\n",
        "Il reste ensuite à entrainer l'agent."
      ],
      "metadata": {
        "id": "LpyPzPQWNUWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a.train(200000, 200, test_callbacks=[show_last_video])"
      ],
      "metadata": {
        "id": "OwHZ63vSNSSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## À vous de jouer !\n",
        "\n",
        "Comparez les résultats obtenus par le DQN avec les résultats obtenus en intégrant diverses avancées en matière de *Deep Reinforcement Learning*. Inspirez vous des méthodes d'évaluation utilisées dans la littérature scientifique pour évaluer les résultats de vos propres expériences.\n",
        "\n",
        "Le code fourni propose déjà des implémentations de plusieurs améliorations proposées dans les articles scientifiques suivants :\n",
        "\n",
        "* [Hasselt, Hado van, Arthur Guez, et David Silver. « Deep Reinforcement Learning with Double Q-learning », 8 décembre 2015](http://arxiv.org/abs/1509.06461)\n",
        "* [Schaul, Tom, John Quan, Ioannis Antonoglou, et David Silver. « Prioritized Experience Replay », 25 février 2016](http://arxiv.org/abs/1511.05952)\n",
        "* [Wang, Ziyu, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, et Nando de Freitas. « Dueling Network Architectures for Deep Reinforcement Learning », 5 avril 2016](http://arxiv.org/abs/1511.06581)\n",
        "\n",
        "Vous devez au minimum :\n",
        "\n",
        "* Réaliser des expériences mettant en avant l'impact individuel de chacune de ces innovations par rapport au DQN fourni ci-dessus. Vous devrez trouver vous-même les classes implémentant ces techniques dans le code fourni (en utilisant par exemple la documentation qui l'accompagne)\n",
        "* Implémenter une innovation présentée dans un article scientifique de votre choix dans une nouvelle classe compatible avec le code fourni. Vous réaliserez une expérience mettant en avant l'impact de cette amélioration par rapport au DQN.\n",
        "\n",
        "Vous pourrez ensuite combiner plusieurs de ces innovations et constater leur impact sur les politiqes apprises par vos agents."
      ],
      "metadata": {
        "id": "MJQlVnpnOr9-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lectures utiles\n",
        "\n",
        "#### Divers\n",
        "\n",
        ">Sutton, Richard S., et Andrew G. Barto. Reinforcement Learning: An Introduction. Second edition. Adaptive Computation and Machine Learning Series. Cambridge, Massachusetts: The MIT Press, 2018. http://incompleteideas.net/book/RLbook2018.pdf\n",
        ">\n",
        "> >Ce livre, bien que ne traitant pas spécifiquement de **Deep** Reinforcement Learning, est un référénce sur le sujet de l'apprentissage par renforcement et présente un grand nombre de techniques.\n",
        "\n",
        "> Deepmind. « Deep Reinforcement Learning ». Consulté le 22 septembre 2021. https://deepmind.com/blog/article/deep-reinforcement-learning.\n",
        ">\n",
        "> > Article de blog de DeepMind datant de 2016 dressant un état de l'art du *Deep Reinforcement Learning* (qui était encore un domaine très jeune) à l'époque. Ce blog est par ailleurs un bon moyen de se tenir informé des derniers travaux d'un des laboratoires les plus prolifiques sur le sujet.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### Articles scientifiques\n",
        "\n",
        "Les articles suivants proposent pour la plupart au moins une inovation en terme de Deep Reinforcement Leaning appliquable aux techniques vues en cours. Certains d'entre eux se consacrent à dresser un état de l'art ou à réaliser des études comparatives entre plusieurs techniques.\n",
        "\n",
        "* Andrychowicz, Marcin, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, et Wojciech Zaremba. « Hindsight Experience Replay ». arXiv:1707.01495 [cs], 23 février 2018. http://arxiv.org/abs/1707.01495.\n",
        " Burda, Yuri, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, et Alexei A. Efros. « Large-Scale Study of Curiosity-Driven Learning ». arXiv:1808.04355 [cs, stat], 13 août 2018. http://arxiv.org/abs/1808.04355.\n",
        "* Christiano, Paul, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, et Dario Amodei. « Deep reinforcement learning from human preferences ». arXiv:1706.03741 [cs, stat], 13 juillet 2017. http://arxiv.org/abs/1706.03741.\n",
        "* Fortunato, Meire, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, et al. « Noisy Networks for Exploration ». arXiv:1706.10295 [cs, stat], 9 juillet 2019. http://arxiv.org/abs/1706.10295.\n",
        "* Hasselt, Hado van, Arthur Guez, et David Silver. « Deep Reinforcement Learning with Double Q-learning ». arXiv:1509.06461 [cs], 8 décembre 2015. http://arxiv.org/abs/1509.06461.\n",
        "* Hessel, Matteo, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, et David Silver. « Rainbow: Combining Improvements in Deep Reinforcement Learning ». arXiv:1710.02298 [cs], 6 octobre 2017. http://arxiv.org/abs/1710.02298.\n",
        "* Hessel, Matteo, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, et David Silver. « Rainbow: Combining Improvements in Deep Reinforcement Learning ». In Thirty-Second AAAI Conference on Artificial Intelligence, 2018. https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17204.\n",
        "* Li, Yuxi. « Deep Reinforcement Learning ». arXiv:1810.06339 [cs, stat], 15 octobre 2018. http://arxiv.org/abs/1810.06339.\n",
        "* Machado, Marlos, Marc Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, et Michael Bowling. « Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents (Extended Abstract) », 5573‑77, 2018. https://doi.org/10.24963/ijcai.2018/787.\n",
        "* Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, et Martin Riedmiller. « Playing Atari with Deep Reinforcement Learning ». arXiv:1312.5602 [cs], 19 décembre 2013. http://arxiv.org/abs/1312.5602.\n",
        "* Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, et al. « Human-Level Control through Deep Reinforcement Learning ». Nature 518, nᵒ 7540 (26 février 2015): 529‑33. https://doi.org/10.1038/nature14236.\n",
        "* Osband, Ian, Charles Blundell, Alexander Pritzel, et Benjamin Van Roy. « Deep Exploration via Bootstrapped DQN ». arXiv:1602.04621 [cs, stat], 4 juillet 2016. http://arxiv.org/abs/1602.04621.\n",
        "*  Pathak, Deepak, Pulkit Agrawal, Alexei A. Efros, et Trevor Darrell. « Curiosity-Driven Exploration by Self-Supervised Prediction ». In 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 488‑89. Honolulu, HI, USA: IEEE, 2017. https://doi.org/10.1109/CVPRW.2017.70.\n",
        "* Plappert, Matthias, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y. Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, et Marcin Andrychowicz. « Parameter Space Noise for Exploration ». arXiv:1706.01905 [cs, stat], 31 janvier 2018. http://arxiv.org/abs/1706.01905.\n",
        "* Schaul, Tom, John Quan, Ioannis Antonoglou, et David Silver. « Prioritized Experience Replay ». arXiv:1511.05952 [cs], 25 février 2016. http://arxiv.org/abs/1511.05952.\n",
        "* Schmid, Martin, Matej Moravcik, Neil Burch, Rudolf Kadlec, Josh Davidson, Kevin Waugh, Nolan Bard, et al. « Player of Games ». arXiv:2112.03178 [cs], 6 décembre 2021. http://arxiv.org/abs/2112.03178.\n",
        "Sutton, Richard S. « Learning to Predict by the Methods of Temporal Differences ». Machine Learning 3, nᵒ 1 (août 1988): 9‑44. https://doi.org/10.1007/BF00115009.\n",
        "* Vecerik, Mel, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nicolas Heess, Thomas Rothörl, Thomas Lampe, et Martin Riedmiller. « Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards ». arXiv:1707.08817 [cs], 8 octobre 2018. http://arxiv.org/abs/1707.08817.\n",
        "* Wang, Ziyu, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, et Nando de Freitas. « Dueling Network Architectures for Deep Reinforcement Learning ». arXiv:1511.06581 [cs], 5 avril 2016. http://arxiv.org/abs/1511.06581.\n"
      ],
      "metadata": {
        "id": "BYvMAqVBlfnV"
      }
    }
  ]
}